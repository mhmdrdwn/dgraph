{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d002a1e-5d1e-4058-a111-8a910a7099fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n",
      "82\n",
      "90\n",
      "1379\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open('../../../caueeg/dementia-no-overlap.json')\n",
    "data_files = json.load(f)\n",
    "f.close()\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "idx2label = {}\n",
    "for file_idx in data_files[\"train_split\"]:\n",
    "    idx2label[file_idx[\"serial\"]] = file_idx[\"class_label\"]\n",
    "for file_idx in data_files[\"test_split\"]:\n",
    "    idx2label[file_idx[\"serial\"]] = file_idx[\"class_label\"]\n",
    "for file_idx in data_files[\"validation_split\"]:\n",
    "    idx2label[file_idx[\"serial\"]] = file_idx[\"class_label\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "import glob, os\n",
    "test_files = []\n",
    "val_files = []\n",
    "train_files = []\n",
    "for file_name in data_files[\"test_split\"]:\n",
    "    test_files.append(\"../../../caueeg/signal/edf/\"+file_name[\"serial\"]+\".edf\")\n",
    "for file_name in data_files[\"train_split\"]:\n",
    "    train_files.append(\"../../../caueeg/signal/edf/\"+file_name[\"serial\"]+\".edf\")\n",
    "for file_name in data_files[\"validation_split\"]:\n",
    "    val_files.append(\"../../../caueeg/signal/edf/\"+file_name[\"serial\"]+\".edf\")\n",
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "print(len(test_files))\n",
    "\n",
    "#event_fname\n",
    "import glob, os\n",
    "all_events_files = []\n",
    "for file in glob.glob(\"../../../caueeg/event/*.json\"):\n",
    "    all_events_files.append(file)\n",
    "print(len(all_events_files))\n",
    "\n",
    "events = {}\n",
    "for file_name in all_events_files:\n",
    "    file = open(file_name)\n",
    "    event = json.load(file)\n",
    "    file_name = \"../../../caueeg/signal/edf/\" + file_name.split(\"/\")[-1][:-5]+\".edf\"\n",
    "    events[file_name] = event\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1a5fb67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import spkit as sp\n",
    "import spkit as sp\n",
    "\n",
    "import scipy.signal as sig\n",
    "from sklearn.decomposition import FastICA\n",
    "    \n",
    "from braindecode.preprocessing import exponential_moving_standardize\n",
    "\n",
    "\n",
    "def get_label(file):\n",
    "    idx = file.split(\"/\")[-1][:-4]\n",
    "    label = idx2label[idx]\n",
    "    if label == 0:\n",
    "        return [0]\n",
    "    elif label == 1:\n",
    "        return [1]\n",
    "    else:\n",
    "        return [2]\n",
    "    \n",
    "    \n",
    "def read_edf_file(file, test=True):\n",
    "    \n",
    "    # load raw data\n",
    "    raw = mne.io.read_raw(file, verbose=False, preload=True)\n",
    "    raw.resample(sfreq=100)\n",
    "    raw.filter(l_freq=1, h_freq=45, verbose=False)\n",
    "    #raw.set_eeg_reference(ref_channels=['Photic'])\n",
    "    \n",
    "    # pick 19 channels\n",
    "    ch_names = ['Fp1', 'F3', 'C3', 'P3', 'O1', 'Fp2', 'F4', 'C4', 'P4', 'O2', 'F7', 'T3', 'T5', 'F8', 'T4', 'T6', 'Fz', 'Pz', 'Cz']\n",
    "    ch_types = ['eeg' for _ in range(len(ch_names))]\n",
    "    info = mne.create_info(ch_names=ch_names, ch_types=ch_types, sfreq=100)\n",
    "    \n",
    "    raw.info = info\n",
    "    picks_eeg = mne.pick_types(raw.info, meg=False, eeg=True, eog=False, stim=False, exclude='bads')\n",
    "\n",
    "    # set montage 10-20\n",
    "    montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "    raw.set_montage(montage)\n",
    "    #raw.filter(1.0, 45.0, fir_design='firwin', n_jobs=1)\n",
    "    #raw.resample(sfreq=100)\n",
    "    #raw = mne.preprocessing.compute_current_source_density(raw)\n",
    "    #raw.filter(1.0, 45.0, fir_design='firwin', n_jobs=1)\n",
    "    raw = raw.get_data()\n",
    "\n",
    "    #raw = (raw - raw.mean())/raw.std()\n",
    "    \n",
    "    #raw = raw.T\n",
    "    #raw = sp.eeg.ICA_filtering(raw.copy(),verbose=0,winsize=100)\n",
    "    #raw = raw.T\n",
    "    #raw = raw.T\n",
    "    #ica = FastICA(n_components=19, whiten=\"unit-variance\")\n",
    "    #raw = ica.fit_transform(raw)\n",
    "    #raw = raw.T    \n",
    "    #raw = exponential_moving_standardize(raw)\n",
    "    raw = mne.filter.filter_data(raw, 100, 1.0, 45.0)\n",
    "    dur = raw.shape[1]\n",
    "    raws = []\n",
    "    if test:\n",
    "        raws.append(raw[:, 5000:10000])\n",
    "    else:\n",
    "        for i in range(3):\n",
    "            if dur > ((i+1)*5000):\n",
    "                raws.append(raw[:, i*5000:(i+1)*5000])\n",
    "    return raws\n",
    "    \n",
    "    \n",
    "def read_edf_file(file, use_windows=False, events=\"Eyes Closed\"):\n",
    "    \n",
    "    # load raw data\n",
    "    raw = mne.io.read_raw(file, verbose=False, preload=True)\n",
    "    raw.filter(l_freq=1, h_freq=45, verbose=False)\n",
    "    #raw.set_eeg_reference(ref_channels=['Photic'])\n",
    "\n",
    "    # pick 19 channels\n",
    "    ch_names = ['Fp1', 'F3', 'C3', 'P3', 'O1', 'Fp2', 'F4', 'C4', 'P4', 'O2', 'F7', 'T3', 'T5', 'F8', 'T4', 'T6', 'Fz', 'Pz', 'Cz']\n",
    "    ch_types = ['eeg' for _ in range(len(ch_names))]\n",
    "    info = mne.create_info(ch_names=ch_names, ch_types=ch_types, sfreq=200)\n",
    "    raw.info = info\n",
    "    picks_eeg = mne.pick_types(raw.info, meg=False, eeg=True, eog=False, stim=False, exclude='bads')\n",
    "\n",
    "    # set montage 10-20\n",
    "    montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "    raw.set_montage(montage)\n",
    "    \n",
    "    # filter\n",
    "    #raw.filter(l_freq=0.5, h_freq=45, verbose=False)\n",
    "    \n",
    "    # events\n",
    "    event_id = {}\n",
    "    idx_ = 0\n",
    "    for event in events[file]:\n",
    "        if not event[1] in event_id.keys():\n",
    "            event_id[event[1]] = idx_\n",
    "            idx_ += 1 \n",
    "            \n",
    "    new_events = []\n",
    "    label_events = []\n",
    "    for event_idx, event in enumerate(events[file]):\n",
    "        new_events.append(event[0])\n",
    "        label_events.append(event_id[event[1]])  \n",
    "        \n",
    "    new_events = np.array(new_events)\n",
    "    new_events = new_events.reshape(new_events.shape[0], 1).astype(int)\n",
    "    zeros = np.array([0 for i in range(new_events.shape[0])]).reshape(new_events.shape[0], 1)\n",
    "    label_events = np.array(label_events).reshape(new_events.shape[0], 1)\n",
    "    new_events = np.concatenate((new_events, zeros, label_events), axis=1)\n",
    "\n",
    "    # read epochs from raw file using events\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=new_events,\n",
    "        event_id=event_id,\n",
    "        picks=picks_eeg,\n",
    "        event_repeated=\"merge\",\n",
    "        preload=True,\n",
    "        tmin=-0.01,\n",
    "        detrend=0,\n",
    "        tmax=30)\n",
    "    \n",
    "    epochs.filter(1.0, 45.0, fir_design='firwin', n_jobs=1)\n",
    "    \n",
    "    # csd of epochs file\n",
    "    #epochs = mne.preprocessing.compute_current_source_density(epochs)\n",
    "    epochs.resample(sfreq=100)\n",
    "\n",
    "    # output epochs data\n",
    "    if epochs.get_data().shape[0] < 5:\n",
    "        return\n",
    "    try:\n",
    "        epochs = epochs[events]\n",
    "        #print(epochs)\n",
    "        epochs_data = epochs.get_data()\n",
    "        epochs_data = epochs_data[:5]\n",
    "        \n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    if not use_windows:\n",
    "        epochs_data = [epochs_data[int(len(epochs_data)/2)]]\n",
    "        #epochs_data = np.array(epochs_data).squeeze()\n",
    "        #epochs_data = epochs_data.T\n",
    "        #epochs_data = sp.eeg.ICA_filtering(epochs_data.copy(),verbose=0,winsize=100)\n",
    "        #epochs_data = epochs_data.T\n",
    "        return epochs_data\n",
    "    else:\n",
    "        return epochs_data\n",
    "    \n",
    "\n",
    "def build_data(raw_data, use_windows=True):\n",
    "    \n",
    "    all_data_features = []\n",
    "    data_labels = []\n",
    "    \n",
    "    for file in tqdm(raw_data):\n",
    "        \n",
    "        #node features\n",
    "        sample_features = []\n",
    "        tfeatures = []\n",
    "        \n",
    "        edf_data = read_edf_file(file, use_windows)\n",
    "        if edf_data is None: \n",
    "            continue\n",
    "            \n",
    "        for edf_data1 in edf_data:\n",
    "            all_data_features.append(edf_data1)\n",
    "            label = get_label(file)\n",
    "            data_labels.append(np.array(label))\n",
    "\n",
    "    all_data_features = np.array(all_data_features)\n",
    "    data_labels = np.array(data_labels)\n",
    "             \n",
    "    return all_data_features, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1006b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#from train import trainer\n",
    "import torch\n",
    "#from tgcn import TGCN\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from scipy import signal\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def multichannel_sliding_window(X, size, step):\n",
    "    shape = (X.shape[0] - X.shape[0] + 1, (X.shape[1] - size + 1) // step, X.shape[0], size)\n",
    "    strides = (X.strides[0], X.strides[1] * step, X.strides[0], X.strides[1])\n",
    "    return np.lib.stride_tricks.as_strided(X, shape, strides)[0]\n",
    "\n",
    "\n",
    "def hilphase(x1,x2):\n",
    "    sig1_hill=sig.hilbert(x1)\n",
    "    sig2_hill=sig.hilbert(x2)\n",
    "    pdt=(np.inner(sig1_hill,np.conj(sig2_hill))/(np.sqrt(np.inner(sig1_hill,\n",
    "               np.conj(sig1_hill))*np.inner(sig2_hill,np.conj(sig2_hill)))))\n",
    "    phase = np.angle(pdt)\n",
    "    return phase\n",
    "    \n",
    "def gc(x1, x2):\n",
    "    X = np.vstack([x1, x2]).T\n",
    "    gc = grangercausalitytests(X, [2], addconst=True, verbose=False)[2][0]['ssr_ftest'][1]\n",
    "    return gc\n",
    "    \n",
    "    \n",
    "# Coherence - δ\n",
    "def coherence(eegData,fs):\n",
    "    coh_res = []\n",
    "    for ii, jj in itertools.combinations(range(eegData.shape[0]), 2):\n",
    "        coh_res.append(CoherenceDelta(eegData, ii, jj, fs=fs))\n",
    "    coh_res = np.array(coh_res)\n",
    "    return coh_res\n",
    "\n",
    "# Mutual information\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def cal_mi(x1, x2):\n",
    "    mi = mutual_info_regression(x1, x2)\n",
    "    return mi\n",
    "\n",
    "# Mutual information\n",
    "def calculate2Chan_MI(eegData,ii,jj,bin_min=-200, bin_max=200, binWidth=2):\n",
    "    H = np.zeros(eegData.shape[2])\n",
    "    bins = np.arange(bin_min+1, bin_max, binWidth)\n",
    "    for epoch in range(eegData.shape[2]):\n",
    "        c_xy = np.histogram2d(eegData[ii,:,epoch],eegData[jj,:,epoch],bins)[0]\n",
    "        H[epoch] = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return H\n",
    "\n",
    "\n",
    "def gen_graphs(eegs, num_nodes=19, cal_conn=\"corr\"):\n",
    "    #eegs(snapshots, bands, timpoints)\n",
    "    c = []\n",
    "    for i in range(num_nodes):\n",
    "        c1 = []\n",
    "        for j in range(num_nodes):\n",
    "            if cal_conn == \"corr\":\n",
    "                conn = pearsonr(eegs[i], eegs[j])[0]\n",
    "                #conn = np.correlate(eegs[i], eegs[j], mode='valid')[0]\n",
    "            elif cal_conn == \"plv\": \n",
    "                conn = hilphase(eegs[i], eegs[j])\n",
    "            elif cal_conn == \"gc\": \n",
    "                conn = gc(eegs[i], eegs[j])\n",
    "            elif cal_conn == \"mi\": \n",
    "                eegsi = eegs[i].reshape(eegs[i].shape[0], 1)\n",
    "                eegsj = eegs[j].reshape(eegs[j].shape[0], 1)\n",
    "                conn = cal_mi(eegsi, eegsj)\n",
    "            elif cal_conn == \"coh\": \n",
    "                conn = coherence(eegs[i], eegs[j])\n",
    "            c1.append(conn)\n",
    "        c.append(c1)\n",
    "    return c\n",
    "\n",
    "\n",
    "def gen_features(X, y, device, cal_conn, window_size=100, overlap=50, augment=False):\n",
    "    X_new = []     \n",
    "    #freqs = np.where((f >= 1) & (f <= 30))\n",
    "\n",
    "    for x in X:\n",
    "        x_ = multichannel_sliding_window(x, window_size, overlap)\n",
    "        X_new.append(x_)\n",
    "        \n",
    "    X_new = np.moveaxis(np.array(X_new), 1, -1)\n",
    "    \n",
    "    graphs = []\n",
    "    dynamic_range = X_new.shape[-1]\n",
    "    \n",
    "    threshold = 0.7\n",
    "\n",
    "    print(\"calculating connectivity\")\n",
    "    for x in tqdm(X_new):\n",
    "        temp = []\n",
    "        for i in range(dynamic_range):\n",
    "            temp_g = gen_graphs(x[:, :, i], cal_conn=cal_conn)\n",
    "            temp_g = np.array(temp_g)\n",
    "            temp_g = (temp_g - temp_g.min())/(temp_g.max() - temp_g.min())\n",
    "            #temp_g[temp_g<threshold] = 0\n",
    "            temp.append(temp_g)\n",
    "        graphs.append(temp)\n",
    "        \n",
    "    graphs = np.array(graphs)\n",
    "    \n",
    "    graphs = np.moveaxis(graphs.squeeze(), 1, -1)\n",
    "            \n",
    "    return X_new, graphs, y\n",
    "\n",
    "import copy\n",
    "def standardize_data(train_X, test_X, val_X):\n",
    "    train_X_std = copy.deepcopy(train_X)\n",
    "    test_X_std = copy.deepcopy(test_X)\n",
    "    val_X_std = copy.deepcopy(val_X)\n",
    "    for i in range(train_X.shape[1]):\n",
    "        min_ = np.min(train_X[:, i, :])\n",
    "        max_ = np.max(train_X[:, i, :])\n",
    "        train_X_std[:, i, :] = (train_X[:, i, :] - min_)/(max_ - min_)\n",
    "        test_X_std[:, i, :] = (test_X[:, i, :] - min_)/(max_ - min_)\n",
    "        val_X_std[:, i, :] = (val_X[:, i, :] - min_)/(max_ - min_)\n",
    "    return train_X_std, test_X_std, val_X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2e73289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def train_test(train_files, test_files, val_files, cal_conn):\n",
    "    print(\"read_data\")\n",
    "    train_X, train_y = build_data(train_files, use_windows=True)\n",
    "    test_X, test_y = build_data(test_files, use_windows=False)\n",
    "    val_X, val_y = build_data(val_files, use_windows=False)\n",
    "    \n",
    "    clear_output()\n",
    "    print(train_X.shape, test_X.shape, val_X.shape)\n",
    "    print(\"standardize_data\")\n",
    "    train_X, test_X, val_X = standardize_data(train_X, test_X, val_X)\n",
    "    \n",
    "    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    train_X, train_graphs, train_y = gen_features(train_X, train_y, DEVICE, cal_conn=cal_conn, \n",
    "                                                  window_size=100, overlap=50, augment=False)\n",
    "    test_X, test_graphs, test_y = gen_features(test_X, test_y, DEVICE, cal_conn=cal_conn, \n",
    "                                               window_size=100, overlap=50, augment=False)\n",
    "    val_X, val_graphs, val_y = gen_features(val_X, val_y, DEVICE, cal_conn=cal_conn, \n",
    "                                               window_size=100, overlap=50, augment=False)\n",
    "    \n",
    "    clear_output()\n",
    "    encoder = OneHotEncoder()\n",
    "    train_y = encoder.fit_transform(train_y).toarray()\n",
    "    test_y = encoder.transform(test_y).toarray()\n",
    "    val_y = encoder.transform(val_y).toarray()\n",
    "    train_X = torch.Tensor(train_X)\n",
    "    test_X = torch.Tensor(test_X)\n",
    "    val_X = torch.Tensor(val_X)\n",
    "    train_y= torch.Tensor(train_y)\n",
    "    test_y = torch.Tensor(test_y)\n",
    "    val_y = torch.Tensor(val_y)\n",
    "    test_graphs= torch.Tensor(test_graphs)\n",
    "    train_graphs = torch.Tensor(train_graphs)\n",
    "    val_graphs = torch.Tensor(val_graphs)\n",
    "    \n",
    "    batch_size = 64\n",
    "    data = TensorDataset(train_X, train_graphs, train_y)\n",
    "    train_iter = torch.utils.data.DataLoader(data, batch_size, shuffle=True)\n",
    "    data = TensorDataset(test_X, test_graphs, test_y)\n",
    "    test_iter = torch.utils.data.DataLoader(data, batch_size, shuffle=False)\n",
    "    data = TensorDataset(val_X, val_graphs, val_y)\n",
    "    val_iter = torch.utils.data.DataLoader(data, batch_size, shuffle=False)\n",
    "    return train_iter, test_iter, val_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63909014",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter, val_iter = train_test(train_files=train_files, \n",
    "                                   test_files=test_files, \n",
    "                                   val_files=val_files, \n",
    "                                   cal_conn=\"corr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0adfcd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "def get_laplacian(edge_index, edge_weight=None, normalization=None, dtype=None,\n",
    "                  num_nodes=19):\n",
    "    \n",
    "    assert normalization in [None, 'sym', 'rw'], 'Invalid normalization'\n",
    "\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones(edge_index.size(1), dtype=dtype,\n",
    "                                 device=edge_index.device)\n",
    "\n",
    "    row, col = edge_index\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "\n",
    "    if normalization is None:\n",
    "        # L = D - A.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "        edge_weight = torch.cat([-edge_weight, deg], dim=0)\n",
    "    elif normalization == 'sym':\n",
    "        # Compute A_norm = -D^{-1/2} A D^{-1/2}.\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "        # L = I - A_norm.\n",
    "        edge_index, edge_weight = add_self_loops(edge_index, -edge_weight,\n",
    "                                                 fill_value=1,\n",
    "                                                 num_nodes=num_nodes)\n",
    "    else:\n",
    "        # Compute A_norm = -D^{-1} A.\n",
    "        deg_inv = 1.0 / deg\n",
    "        deg_inv[deg_inv == float('inf')] = 0\n",
    "        edge_weight = deg_inv[row] * edge_weight\n",
    "\n",
    "        # L = I - A_norm.\n",
    "        edge_index, edge_weight = add_self_loops(edge_index, -edge_weight,\n",
    "                                                 fill_value=1,\n",
    "                                                 num_nodes=num_nodes)\n",
    "\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, auc, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TGCN2(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 improved: bool = False, cached: bool = False, \n",
    "                 add_self_loops: bool = True):\n",
    "        super(TGCN2, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self._create_parameters_and_layers()\n",
    "\n",
    "    def _create_update_gate_parameters_and_layers(self):\n",
    "        self.conv_z = GCNConv(in_channels=self.in_channels,  out_channels=self.out_channels, improved=self.improved,\n",
    "                              cached=self.cached, add_self_loops=self.add_self_loops )\n",
    "        self.linear_z = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_reset_gate_parameters_and_layers(self):\n",
    "        self.conv_r = GCNConv(in_channels=self.in_channels, out_channels=self.out_channels, improved=self.improved,\n",
    "                              cached=self.cached, add_self_loops=self.add_self_loops )\n",
    "        self.linear_r = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_candidate_state_parameters_and_layers(self):\n",
    "        self.conv_h = GCNConv(in_channels=self.in_channels, out_channels=self.out_channels, improved=self.improved,\n",
    "                              cached=self.cached, add_self_loops=self.add_self_loops )\n",
    "        self.linear_h = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_parameters_and_layers(self):\n",
    "        self._create_update_gate_parameters_and_layers()\n",
    "        self._create_reset_gate_parameters_and_layers()\n",
    "        self._create_candidate_state_parameters_and_layers()\n",
    "\n",
    "    def _set_hidden_state(self, X, H):\n",
    "        if H is None:\n",
    "            # can infer batch_size from X.shape, because X is [B, N, F]\n",
    "            H = torch.zeros(X.shape[0], X.shape[1], self.out_channels).to(X.device) #(b, 207, 32)\n",
    "        return H\n",
    "\n",
    "    def _calculate_update_gate(self, X, edge_index, edge_weight, H):\n",
    "        Z = torch.cat([self.conv_z(X, edge_index, edge_weight), H], axis=2) # (b, 207, 64)\n",
    "        Z = self.linear_z(Z) # (b, 207, 32)\n",
    "        Z = torch.sigmoid(Z)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def _calculate_reset_gate(self, X, edge_index, edge_weight, H):\n",
    "        R = torch.cat([self.conv_r(X, edge_index, edge_weight), H], axis=2) # (b, 207, 64)\n",
    "        R = self.linear_r(R) # (b, 207, 32)\n",
    "        R = torch.sigmoid(R)\n",
    "\n",
    "        return R\n",
    "\n",
    "    def _calculate_candidate_state(self, X, edge_index, edge_weight, H, R):\n",
    "        H_tilde = torch.cat([self.conv_h(X, edge_index, edge_weight), H * R], axis=2) # (b, 207, 64)\n",
    "        H_tilde = self.linear_h(H_tilde) # (b, 207, 32)\n",
    "        H_tilde = torch.tanh(H_tilde)\n",
    "\n",
    "        return H_tilde\n",
    "\n",
    "    def _calculate_hidden_state(self, Z, H, H_tilde):\n",
    "        H = Z * H + (1 - Z) * H_tilde   # # (b, 207, 32)\n",
    "        return H\n",
    "\n",
    "    def forward(self,X: torch.FloatTensor, edge_index: torch.LongTensor, edge_weight: torch.FloatTensor = None,\n",
    "                H: torch.FloatTensor = None ) -> torch.FloatTensor:\n",
    "       \n",
    "        H = self._set_hidden_state(X, H)\n",
    "        Z = self._calculate_update_gate(X, edge_index, edge_weight, H)\n",
    "        R = self._calculate_reset_gate(X, edge_index, edge_weight, H)\n",
    "        H_tilde = self._calculate_candidate_state(X, edge_index, edge_weight, H, R)\n",
    "        H = self._calculate_hidden_state(Z, H, H_tilde) # (b, 207, 32)\n",
    "        return H\n",
    "    \n",
    "\n",
    "\n",
    "class A3TGCN2(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int, \n",
    "        out_channels: int,  \n",
    "        periods: int, \n",
    "        improved: bool = False,\n",
    "        cached: bool = False,\n",
    "        add_self_loops: bool = True):\n",
    "        super(A3TGCN2, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels  # 2\n",
    "        self.out_channels = out_channels # 32\n",
    "        self.periods = periods # 12\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self._setup_layers()\n",
    "\n",
    "    def _setup_layers(self):\n",
    "        self._base_tgcn = TGCN2(\n",
    "            in_channels=3,\n",
    "            out_channels=self.out_channels,  \n",
    "            improved=self.improved,\n",
    "            cached=self.cached, \n",
    "            add_self_loops=self.add_self_loops)\n",
    "\n",
    "        self.Encoder =  Encoder(timepoints=100, num_nodes=19)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._attention = torch.nn.Parameter(torch.empty(self.periods, device=device))\n",
    "        torch.nn.init.uniform_(self._attention)\n",
    "\n",
    "    def forward( \n",
    "        self, \n",
    "        X: torch.FloatTensor,\n",
    "        A: torch.FloatTensor, \n",
    "        edge_weight: torch.FloatTensor = None,\n",
    "        H: torch.FloatTensor = None\n",
    "    ) -> torch.FloatTensor:\n",
    "\n",
    "        \n",
    "        Xenc = self.Encoder(X)\n",
    "        H_accum = 0\n",
    "        probs = torch.nn.functional.softmax(self._attention, dim=0)\n",
    "\n",
    "        for period in range(self.periods):\n",
    "            Xt = Xenc[:, :, :, period]\n",
    "            At = A[:, :, :, period]\n",
    "            A_diag = torch.block_diag(*At)\n",
    "            shape = A_diag.shape[0]\n",
    "            idx = torch.ones(2, shape*shape).long().to(X.device)\n",
    "            w = A_diag.flatten().float().to(X.device)\n",
    "            idx, w = get_laplacian(idx, w, normalization=\"rw\")\n",
    "            H_accum = H_accum + probs[period] * self._base_tgcn(Xt, idx, w, H) #([32, 207, 32]\n",
    "        return H_accum\n",
    "        \n",
    "class EEGModel(nn.Module):\n",
    "    def __init__(self, num_nodes, node_features, num_classes, num_windows, device):\n",
    "        super(EEGModel, self).__init__()\n",
    "        self.a3tgnn = A3TGCN2(in_channels=node_features, out_channels=32, periods=num_windows) # node_features=2, periods=12\n",
    "        self.num_nodes = num_nodes\n",
    "        self.BN = nn.BatchNorm1d(self.num_nodes)\n",
    "        self.num_windows = num_windows\n",
    "        self.fc2 = nn.Linear(self.num_nodes*32, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(-1)\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        HS = self.a3tgnn(X, A)\n",
    "        HS = nn.functional.relu(HS)\n",
    "        HS = self.BN(HS)\n",
    "        HS = HS.reshape(HS.shape[0], self.num_nodes*32)\n",
    "        out = self.fc2(HS)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, timepoints, num_nodes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_nodes, 64, kernel_size=(3, 1), stride=(2, 1))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.fc1 = nn.Linear(128, num_nodes)\n",
    "        self.fc2 = nn.Linear(5, 5)\n",
    "        #self.pooling = torch.nn.AdaptiveAvgPool2d((19, 3))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.permute(0, 3, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = self.fc2(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, timepoints, num_nodes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_nodes, 32, kernel_size=(3, 1), stride=(2, 1))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 1), stride=(2, 1))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        self.pooling = torch.nn.AdaptiveAvgPool2d((num_nodes, 3))\n",
    "        self.BN1 = nn.BatchNorm2d(64)\n",
    "        self.BN2 = nn.BatchNorm2d(num_nodes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.BN1(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.pooling(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.BN2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20098f62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████▍   | 66/72 [01:37<00:09,  1.51s/it]"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_iter, val_iter, num_epochs, lr):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    opt_loss = np.inf\n",
    "    print(\"Training model\")\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = 0\n",
    "        for idx, (X, A, y) in enumerate(tqdm(train_iter)):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X, A)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss.item()*X.shape[0]\n",
    "        print(\"Epoch \", epoch+1, \":\")\n",
    "        print(\"train loss:\", losses/len(train_iter.dataset))\n",
    "\n",
    "        losses = 0\n",
    "        for idx, (X, A, y) in enumerate(val_iter):\n",
    "            out = model(X, A)\n",
    "            loss = criterion(out, y)\n",
    "            losses += loss.item()*X.shape[0]\n",
    "        print(\"val loss:\", losses/len(val_iter.dataset))\n",
    "\n",
    "        if losses/len(val_iter.dataset) < opt_loss:\n",
    "            torch.save(model.state_dict(), \"saved_model/model.pt\")\n",
    "            opt_loss = losses/len(val_iter.dataset) \n",
    "\n",
    "model = EEGModel(num_nodes=19, node_features=100, num_classes=3, num_windows=58, device=DEVICE)\n",
    "train_model(model, train_iter, val_iter, num_epochs=20, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae9c11ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5955056179775281 [[16  6  3]\n",
      " [16 22  4]\n",
      " [ 2  5 15]]\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model_path, data_iter):\n",
    "    outs= []\n",
    "    ys = []\n",
    "\n",
    "    model = EEGModel(num_nodes=19, node_features=100, num_classes=3, num_windows=58, device=DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=False))   \n",
    "\n",
    "    for X, A, y in data_iter:\n",
    "        out = model(X, A)\n",
    "        outs.extend(out.cpu().detach().numpy())\n",
    "        ys.extend(y.cpu().detach().numpy())\n",
    "\n",
    "    outs = np.array(outs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    print(accuracy_score(np.argmax(outs, -1), np.argmax(ys, -1)), \n",
    "          confusion_matrix(np.argmax(outs, -1), np.argmax(ys, -1)))\n",
    "    \n",
    "eval_model(\"saved_model/model.pt\", test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e6335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
